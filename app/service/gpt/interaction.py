import os
import subprocess
from datetime import datetime

import pyaudio
import numpy as np
from faster_whisper import WhisperModel
from openai import OpenAI
from elevenLabs import text_to_speech_file
from elevenlabs import ElevenLabs
from dotenv import load_dotenv

# ì•„ë˜ ë‘ í•¨ìˆ˜ëŠ” record_respberry.py ì— êµ¬í˜„ëœ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# emotion_record(index) â†’ "{prefix}{index}.wav" íŒŒì¼ì„ ë§Œë“¤ì–´ ë¦¬í„´
# is_silent(data) â†’ ìŒì„± ì²­í¬ê°€ ì¹¨ë¬µì¸ì§€ ì—¬ë¶€ íŒë‹¨
from record_respberry import emotion_record, is_silent

# ==== ê³µí†µ ì„¤ì • ====
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ELEVENLABS_KEY   = os.getenv("ELEVENLABS_KEY")

if not OPENAI_API_KEY or not ELEVENLABS_KEY:
    raise RuntimeError(".env ì— OPENAI_API_KEY/ELEVENLABS_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”")

# OpenAI / ElevenLabs í´ë¼ì´ì–¸íŠ¸
gpt_client   = OpenAI(api_key=OPENAI_API_KEY)
tts_client   = ElevenLabs(api_key=ELEVENLABS_KEY)

# Whisper ëª¨ë¸ (tiny, CPU, int8)
whisper_model = WhisperModel("tiny", device="cpu", compute_type="int8")

# ë…¹ìŒ íŒŒë¼ë¯¸í„° (ALSA default=USBMIC ìœ¼ë¡œ ì¡íŒ ìƒíƒœ)
FORMAT   = pyaudio.paInt16
CHANNELS = 1
RATE     = 44100
CHUNK    = RATE * 3    # 3ì´ˆ ë‹¨ìœ„ ë²„í¼

# ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ë°˜ ë…¹ìŒ íŒŒì¼ ì €ì¥ ê²½ë¡œ prefix
today_str           = datetime.now().strftime("%Y%m%d")
WAVE_OUTPUT_PREFIX  = f"/home/team4/Desktop/capstone/AI/app/emotion_diary/{today_str}_"

def interaction(alias: str):
    """
    alias: ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” AIê°€ ë¶€ë¥´ëŠ” ë³„ì¹­ (ex: "í™ê¸¸ë™")
    1) alias ì¸ì‚¬ â†’ TTS â†’ ì¬ìƒ
    2) ì´í›„ ë°˜ë³µ: emotion_record â†’ Whisper STT â†’ GPT ì§ˆë¬¸ ìƒì„± â†’ TTS â†’ ì¬ìƒ
    """
    # 1) alias ì¸ì‚¬
    greet_text = f"{alias}~~ ì˜¤ëŠ˜ ì¢‹ì€ í•˜ë£¨ ë³´ëƒˆë‚˜~~?? ì–´ë–»ê²Œ ì§€ëƒˆì–´!!"
    print("ğŸ‘‹ ì¸ì‚¬:", greet_text)
    greet_audio = text_to_speech_file(greet_text)
    subprocess.run(["mpg321", greet_audio], check=True)

    # ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”
    messages = [
        {"role": "system",
         "content": "ë„ˆëŠ” ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ëŠ” AIì•¼. ì‚¬ìš©ìì™€ ê³„ì† ì´ì–´ì§€ëŠ” ëŒ€í™”ë¥¼ ë§Œë“¤ì–´ì•¼ í•´."},
        {"role": "assistant", "content": greet_text}
    ]

    record_idx = 0
    try:
        while True:
            # 2-1) ê°ì • ë…¹ìŒ (ì¹¨ë¬µ ê¸°ì¤€ìœ¼ë¡œ ìë™ ì¢…ë£Œ)
            wav_path = emotion_record(record_idx)
            print(f"[ë…¹ìŒ ì™„ë£Œ] {wav_path}")
            record_idx += 1

            # 2-2) Whisper STT (í•œêµ­ì–´)
            segments, _ = whisper_model.transcribe(wav_path,
                                                   beam_size=1,
                                                   language="ko")
            user_text = " ".join(seg.text for seg in segments).strip()
            print("â–¶ ì‚¬ìš©ì ìŒì„±(í…ìŠ¤íŠ¸):", user_text or "(ì¸ì‹ ì•ˆë¨)")

            if not user_text:
                print("(ìŒì„± ì¸ì‹ ì‹¤íŒ¨ â†’ ë‹¤ì‹œ ë…¹ìŒ)")
                continue

            # 2-3) GPT-4o ì— ì§ˆë¬¸ ìƒì„± ìš”ì²­
            messages.append({"role": "user", "content": user_text})
            resp = gpt_client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
            question = resp.choices[0].message.content.strip()
            print("ìƒì„±ëœ ì§ˆë¬¸:", question)

            # 2-4) ëŒ€í™” ì´ë ¥ì— ì–´ì‹œìŠ¤í„´íŠ¸ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "assistant", "content": question})

            # 2-5) ì§ˆë¬¸ â†’ ElevenLabs TTS â†’ íŒŒì¼
            tts_path = text_to_speech_file(question)
            print("  (TTS íŒŒì¼ ìƒì„±:", tts_path, ")")

            # 2-6) ì¬ìƒ
            subprocess.run(["mpg321", tts_path], check=True)

    except KeyboardInterrupt:
        print("\n[ì‚¬ìš©ì ì¢…ë£Œ ìš”ì²­] interactionì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print("ì˜ˆì™¸ ë°œìƒ:", e)

    print("=== interaction ì¢…ë£Œ ===")

if __name__ == "__main__":
    # ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ë™ì‘
    # aliasë¥¼ ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”
    interaction("í™ê¸¸ë™")

